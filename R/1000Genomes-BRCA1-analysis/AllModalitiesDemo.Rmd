<!-- R Markdown Documentation, DO NOT EDIT THE PLAIN MARKDOWN VERSION OF THIS FILE -->

<!-- Copyright 2014 Google Inc. All rights reserved. -->

<!-- Licensed under the Apache License, Version 2.0 (the "License"); -->
<!-- you may not use this file except in compliance with the License. -->
<!-- You may obtain a copy of the License at -->

<!--     http://www.apache.org/licenses/LICENSE-2.0 -->

<!-- Unless required by applicable law or agreed to in writing, software -->
<!-- distributed under the License is distributed on an "AS IS" BASIS, -->
<!-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -->
<!-- See the License for the specific language governing permissions and -->
<!-- limitations under the License. -->

Data Analyis using the Genomics API
===================================

The following example makes use of the [Phase 1 variants](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/README.phase1_integrated_release_version3_20120430) from the [1,000 Genomes Project](http://www.1000genomes.org/).  For more detail about how this data was loaded into the Google Genomics API, please see [Google Genomics Public Data](http://developers.google.com/genomics/datasets/1000-genomes-phase-1).

The VCFs comprising this dataset are **3.5 TB** when uncompressed and provide information about **39,706,715** variants for **1,092** individuals.

```{r init, cache=FALSE, comment=NA, echo=FALSE, message=FALSE, warning=FALSE}
# TODO: ensure that bigrquery is released to CRAN
require(bigrquery)
require(ggplot2)
require(dplyr)
require(xtable)
require(testthat)
require(scales)

# Setup for BigQuery access
project <- "genomics-public-data" # put your projectID here
DisplayAndDispatchQuery <- function(queryUri, replacements=list()) {
  querySql <- readChar(queryUri, nchars=1e6)
  cat(querySql)
  for(replacement in names(replacements)) {
    querySql <- sub(replacement, replacements[[replacement]], querySql, fixed=TRUE)
  }
  query_exec(querySql, project)
}

# Setup for Genomics API access
library(GoogleGenomics)
GoogleGenomics::authenticate(file="~/.ssh/genomics_client_secrets.json")
```

Working at Scale
-------------------

### Cluster Computing

Suppose we have a new dataset.  One of the first things we might do is a basic visualization.  Let's start by projecting the relevant data into 2-dimensional space by performing a [Principal Coordinate Analysis](http://occamstypewriter.org/boboh/2012/01/17/pca_and_pcoa_explained/) based on the number of variants shared by each pair of individuals.
```{r cache=FALSE, comment=NA}
# TODO: In live demo, spin up Spark cluster on Google Compute Engine Click-to-deploy
# and kick off full PCoA job from R but then load in pre-computed results so that 
# we can proceed.
pca_1kg <- read.table("./data/1kg-pca.tsv", col.names=c("Sample", "PC1", "PC2"))
# TODO: also consider just computing the similarity matrix via Spark and use the
# appropriate R package to compute PCA
```
This analysis performed an $O(N^2)$ computation upon the relevant fields within the *3.5 TB* of data by running an [Apache Spark](http://spark.apache.org/) job which used the [Genomics Variants API](https://developers.google.com/genomics/v1beta/reference/variants) for its input.  When running upon X cores, this job typically takes Y minutes. 

Visualizing the results we see quite distinct clusters:
```{r fig.align="center", fig.width=10, cache=FALSE, comment=NA}
ggplot(pca_1kg) +
  geom_point(aes(x=PC1, y=PC2)) +
  xlab("principal component 1") +
  ylab("principal component 2") +
  ggtitle("Principal Coordinate Analysis upon 1,000 Genomes")
```

Let's pull in the [supplementary information](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/working/20130606_sample_info/README_20130606_sample_info) we do have on these samples from [Google Cloud Storage](https://developers.google.com/storage/):
```{r cache=FALSE, comment=NA}
sample_info <- read.csv("http://storage.googleapis.com/genomics-public-data/1000-genomes/other/sample_info/sample_info.csv")
pca_1kg <- inner_join(pca_1kg, sample_info)
```

```{r echo=FALSE, eval=FALSE, cache=FALSE, comment=NA}
# We could have also gotten this data from BigQuery
sample_info <- DisplayAndDispatchQuery("./sql/sample-info.sql")
pca_1kg <- inner_join(pca_1kg, sample_info)
```

Applying sample ethnicity to the plot:
```{r fig.align="center", fig.width=10, cache=FALSE, comment=NA}
ggplot(pca_1kg) +
  geom_point(aes(x=PC1, y=PC2, color=Super_Population)) +
  xlab("principal component 1") +
  ylab("principal component 2") +
  ggtitle("Principal Coordinate Analysis upon 1,000 Genomes")
```

we see that ethnicity appears to be the primary explanation for the clusters.

### Querying

Let's also validate this by delving futher into counts of heterozygous reference (where one of the alleles is equal to the reference) and heterozygous 
alternate (where neither of the alleles is equal to the reference) variants.
```{r cache=FALSE, comment=NA}
sample_alt_counts <- DisplayAndDispatchQuery("./sql/sample-alt-counts.sql")
```
This analysis performed an $O(N)$ computation via [Google BigQuery](https://developers.google.com/bigquery/).  Since BigQuery is a columnar data store, it scans only the columns referenced by the query.  In this case, 1 TB of data was scanned, typically within 10 seconds.

Visualizing the results we again see quite distinct clusters:
```{r fig.align="center", fig.width=10, cache=FALSE, comment=NA}
sample_alt_counts <- inner_join(sample_alt_counts, sample_info)
ggplot(sample_alt_counts) +
  geom_point(aes(x=single, y=double, color=Super_Population)) +
  scale_x_continuous(label=scientific_format()) +
  scale_y_continuous(label=scientific_format()) +
  xlab("Variants with a single non-reference allele") +
  ylab("Variants with two non-reference alleles") +
  ggtitle("Heterozygosity Counts within 1,000 Genomes")
```

Zooming-In
------------------------

Suppose we are interested in examining variants within the BRCA1 gene.  We might run our PCoA a second time, zooming-in specifically to this region within the genome.

```{r cache=FALSE, comment=NA}
# TODO: in live demo, run Spark locally and kick off PCoA job from R.
pca_1kg_brca1 <- read.table("./data/1kg-brca1-pca.tsv", col.names=c("Sample", "PC1", "PC2"))
```
Since the amount of data over which this Spark job ran was small, it was feasible to run it locally.

Examining this data visually:
```{r fig.align="center", fig.width=10, cache=FALSE, comment=NA}
ggplot(pca_1kg_brca1) +
  geom_point(aes(x=PC1, y=PC2)) +
  xlab("principal component 1") +
  ylab("principal component 2") +
  ggtitle("Principal Coordinate Analysis upon BRCA1 within 1,000 Genomes")
```

we see distinct clusters with a much different structure than our former result upon the entire dataset.  

Let's apply the sample information we have to this visualization to see if any of it explains the clustering.
```{r fig.align="center", fig.width=10, cache=FALSE, comment=NA}
pca_1kg_brca1 <- inner_join(pca_1kg_brca1, sample_info)
ggplot(pca_1kg_brca1) +
  geom_point(aes(x=PC1, y=PC2, color=Gender)) +
  xlab("principal component 1") +
  ylab("principal component 2") +
  ggtitle("Principal Coordinate Analysis upon BRCA1 within 1,000 Genomes")
```
Gender has no apparent bearing on these variants.

```{r fig.align="center", fig.width=10, cache=FALSE, comment=NA}
ggplot(pca_1kg_brca1) +
  geom_point(aes(x=PC1, y=PC2, color=Super_Population)) +
  xlab("principal component 1") +
  ylab("principal component 2") +
  ggtitle("Principal Coordinate Analysis upon BRCA1 within 1,000 Genomes")
```

we see that ethnicity does appear to account for some amount of the clustering in the second principal component axis but not in the first principal component axis.

Let's split these individuals into two groups based on their position relative to the origin of the first principal component and visualize them again with their grouping.
```{r cache=FALSE, comment=NA}
pca_1kg_brca1 <- mutate(pca_1kg_brca1, 
                        case = 0 > PC1)
```
```{r fig.align="center", fig.width=10, cache=FALSE, comment=NA}
ggplot(pca_1kg_brca1) +
  geom_point(aes(x=PC1, y=PC2, color=Super_Population, shape=case), size=3) +
  xlab("principal component 1") +
  ylab("principal component 2") +
  ggtitle("Principal Coordinate Analysis upon BRCA1 within 1,000 Genomes")
```

Next we perform a simplistic GWAS on the BRCA1 variants to retreive a ranked list of variants that differentiate these groups.
```{r cache=FALSE, comment=NA}
case_sample_ids <- paste("'", filter(pca_1kg_brca1, case==TRUE)$Sample, "'", sep="", collapse=",")
significant_variants <- DisplayAndDispatchQuery("./sql/gwas-brca1-pattern.sql",
                                                list(CASE_SAMPLE_IDS__=case_sample_ids))
```
Note that with a minor change to the SQL, we could have run this same GWAS query over all variants within a much larger region, over an entire chromosome, or even the full dataset; returning the ranked list of variants that differ between the two groups.
```{r cache=FALSE, comment=NA}
head(significant_variants)
```

Next we will annotate the top differenting variants using [BioConductor](http://www.bioconductor.org/).  First we will use the Genomics API R client to retrieve just the variant in which we are interested and expose it to R using the BioConductor VRanges data type.
```{r cache=FALSE, comment=NA}
variantData <- getVariantData(datasetId="10473108253681171589", chromosome="17", start=41218332, end=41218333)
summary(variantData)
```

```{r cache=FALSE, comment=NA, message=FALSE}
require(VariantAnnotation)
require(BSgenome.Hsapiens.UCSC.hg19)
require(TxDb.Hsapiens.UCSC.hg19.knownGene)
txdb <- TxDb.Hsapiens.UCSC.hg19.knownGene
variantData <- renameSeqlevels(variantData, c("17"="chr17"))
# TODO: talk to the BioConductor team at 1:30pm to get advice on what might be a better example here
all <- locateVariants(variantData, txdb, AllVariants())
all
# TODO: see if the structure we see here corresponds to hapotypes http://hapmap.ncbi.nlm.nih.gov/originhaplotype.html
```

And if we want to zoom in even further, we can retrieve the reads from the [Genomics Reads API](https://developers.google.com/genomics/v1beta/reference/readsets) for a given sample and examine coverage:
```{r cache=FALSE, comment=NA}
readData <- getReadData(readsetId="CMvnhpKTFhCxoo_umsDk4CA", chromosome="17", start=41218200, end=41218500)
summary(readData)
```

```{r fig.align="center", fig.width=10, cache=FALSE, comment=NA}
require(ggbio)
p1 <- autoplot(readData, aes(color=strand, fill=strand))
p2 <- ggplot(as(readData, "GRanges")) + stat_coverage(color="gray40", fill="skyblue")
tracks(p1, p2, xlab="chr17")
```

See also [GABrowse](http://gabrowse.appspot.com/#=&readsetId=CJDmkYn8ChCcnc7i4KaWqmQ&backend=GOOGLE) for an interactive Reads browser.

In summary, in this demo from the R prompt we were able to exercise both large scale and small scale data analysis using cloud-based infrastructure.

Provenance
-------------------
Lastly, let us capture version information about R and loaded packages for the sake of provenance.
```{r cache=FALSE, comment=NA}
sessionInfo()
```

```{r, include=FALSE}
   file.rename(from="AllModalitiesDemo.md", 
               to="README.md")
```
